{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"e:/ML_practice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Import \n",
    "import numerical_approximation as na\n",
    "from deep_neural_network import deep_nn as nn\n",
    "\n",
    "# Library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3)\n",
    "y = np.array([[1, 1, 0]])\n",
    "layer_dims = [4,5,3,1]\n",
    "W1 = np.random.randn(5, 4)\n",
    "b1 = np.random.randn(5, 1)\n",
    "W2 = np.random.randn(3, 5)\n",
    "b2 = np.random.randn(3, 1)\n",
    "W3 = np.random.randn(1, 3)\n",
    "b3 = np.random.randn(1, 1)\n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aL, caches = nn.L_model_forward(x, parameters)\n",
    "gradients = nn.L_model_backward(aL, y, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(parameters, X, y, grads, layer_dims, epsilon = 1e-7):\n",
    "    grad_approx = na.numerical_approximation(parameters, X, y, layer_dims)\n",
    "    grad, _ = na.gradient_to_vector(gradients, layer_dims)\n",
    "\n",
    "\n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator/ denominator\n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print(\"\\033[93m\" + f\"There is a mistake in the backward propagation! difference = {difference}\"  + \"\\033[0m\")\n",
    "    else:\n",
    "        print(\"\\033[93m\" + f\"Gradient Descent is working correctly with differnce {difference}\"  + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mGradient Descent is working correctly with differnce 1.189041787835864e-07\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.189041787835864e-07"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_checking(parameters, x, y , gradients, layer_dims)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7287e2b844fdb9e9bb8df4208c5e278243b235ce7a3c7cba557f07ad1b5eb558"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
